{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b792878",
   "metadata": {},
   "source": [
    "This is check the progress for encoding all 128 configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9a57f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 experiment configs\n",
      "{'path': '/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=4_alpha=32/2wikimultihopqa/lr=0.0003_epoch=4_direct/aug_model=qwen2.5-1.5b-instruct/compositional', 'encoded': 300, 'rank': '4', 'alpha': '32', 'lr': '0.0003', 'epoch': '4', 'cot_name': 'direct', 'augment_model': 'qwen2.5-1.5b-instruct', 'model_name': 'qwen2.5-1.5b-instruct', 'dataset': '2wikimultihopqa', 'filename': 'compositional'}\n",
      "{'path': '/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=4_alpha=32/2wikimultihopqa/lr=0.0003_epoch=4_direct/aug_model=qwen2.5-1.5b-instruct/inference', 'encoded': 300, 'rank': '4', 'alpha': '32', 'lr': '0.0003', 'epoch': '4', 'cot_name': 'direct', 'augment_model': 'qwen2.5-1.5b-instruct', 'model_name': 'qwen2.5-1.5b-instruct', 'dataset': '2wikimultihopqa', 'filename': 'inference'}\n",
      "{'path': '/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=4_alpha=32/2wikimultihopqa/lr=0.0003_epoch=4_direct/aug_model=qwen2.5-1.5b-instruct/comparison', 'encoded': 300, 'rank': '4', 'alpha': '32', 'lr': '0.0003', 'epoch': '4', 'cot_name': 'direct', 'augment_model': 'qwen2.5-1.5b-instruct', 'model_name': 'qwen2.5-1.5b-instruct', 'dataset': '2wikimultihopqa', 'filename': 'comparison'}\n",
      "{'path': '/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=4_alpha=32/2wikimultihopqa/lr=0.0003_epoch=4_direct/aug_model=qwen2.5-1.5b-instruct/bridge_comparison', 'encoded': 300, 'rank': '4', 'alpha': '32', 'lr': '0.0003', 'epoch': '4', 'cot_name': 'direct', 'augment_model': 'qwen2.5-1.5b-instruct', 'model_name': 'qwen2.5-1.5b-instruct', 'dataset': '2wikimultihopqa', 'filename': 'bridge_comparison'}\n",
      "{'path': '/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=4_alpha=32/2wikimultihopqa/lr=0.0003_epoch=2_direct/aug_model=qwen2.5-1.5b-instruct/comparison', 'encoded': 300, 'rank': '4', 'alpha': '32', 'lr': '0.0003', 'epoch': '2', 'cot_name': 'direct', 'augment_model': 'qwen2.5-1.5b-instruct', 'model_name': 'qwen2.5-1.5b-instruct', 'dataset': '2wikimultihopqa', 'filename': 'comparison'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "OFFLINE_DIR = \"/scratch/doluk/Compact-Interference-PRAG/offline\"\n",
    "\n",
    "\n",
    "def extract_args_from_filename_dir(path: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Extract experiment arguments from a path ending at:\n",
    "    .../aug_model=<aug_model>/<filename>\n",
    "\n",
    "    Only accept lr=..._epoch=..._direct\n",
    "    Skip warm_up\n",
    "    \"\"\"\n",
    "    # Skip warm_up explicitly\n",
    "    if \"/warm_up/\" in path:\n",
    "        return None\n",
    "\n",
    "    parts = os.path.normpath(path).split(os.sep)\n",
    "    args = {}\n",
    "\n",
    "    for p in parts:\n",
    "        # rank=2_alpha=32\n",
    "        if p.startswith(\"rank=\"):\n",
    "            for item in p.split(\"_\"):\n",
    "                if \"=\" in item:\n",
    "                    k, v = item.split(\"=\", 1)\n",
    "                    args[k] = v\n",
    "\n",
    "        # lr=0.0003_epoch=4_direct  (ONLY accept _direct)\n",
    "        elif p.startswith(\"lr=\"):\n",
    "            segs = p.split(\"_\")\n",
    "            if len(segs) != 3 or segs[2] != \"direct\":\n",
    "                return None\n",
    "\n",
    "            if \"=\" in segs[0]:\n",
    "                k, v = segs[0].split(\"=\", 1)\n",
    "                args[k] = v\n",
    "\n",
    "            if \"=\" in segs[1]:\n",
    "                k, v = segs[1].split(\"=\", 1)\n",
    "                args[k] = v\n",
    "\n",
    "            args[\"cot_name\"] = \"direct\"\n",
    "\n",
    "        # aug_model=...\n",
    "        elif p.startswith(\"aug_model=\"):\n",
    "            args[\"augment_model\"] = p.split(\"=\", 1)[1]\n",
    "\n",
    "    # model_name = directory immediately before rank=\n",
    "    for i, p in enumerate(parts):\n",
    "        if p.startswith(\"rank=\") and i > 0:\n",
    "            args[\"model_name\"] = parts[i - 1]\n",
    "            break\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # dataset = directory immediately after rank=\n",
    "    for i, p in enumerate(parts):\n",
    "        if p.startswith(\"rank=\") and i + 1 < len(parts):\n",
    "            args[\"dataset\"] = parts[i + 1]\n",
    "            break\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # filename is the last directory (total / comparison / bridge / inference / etc.)\n",
    "    args[\"filename\"] = parts[-1]\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def collect_experiments(root_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Walk OFFLINE_DIR and collect unique experiment folders\n",
    "    stopping at filename level (parent of data_*)\n",
    "    \"\"\"\n",
    "    experiments = []\n",
    "    seen = set()\n",
    "\n",
    "    for root, dirs, _ in os.walk(root_dir):\n",
    "        # Identify filename directories by presence of data_* children\n",
    "        if any(d.startswith(\"data_\") for d in dirs):\n",
    "            if root in seen:\n",
    "                continue\n",
    "            seen.add(root)\n",
    "\n",
    "            args = extract_args_from_filename_dir(root)\n",
    "            if args is None:\n",
    "                continue\n",
    "\n",
    "            experiments.append({\n",
    "                \"path\": root,\n",
    "                \"encoded\": os.listdir(root).__len__(),\n",
    "                **args\n",
    "            })\n",
    "\n",
    "            # Do not descend into data_* directories\n",
    "            dirs[:] = [d for d in dirs if not d.startswith(\"data_\")]\n",
    "\n",
    "    return experiments\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiments = collect_experiments(OFFLINE_DIR)\n",
    "\n",
    "    print(f\"Found {len(experiments)} experiment configs\")\n",
    "\n",
    "    # sanity check\n",
    "    for e in experiments[:5]:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bfc8e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=4_alpha=32/2wikimultihopqa/lr=0.0003_epoch=4_direct/aug_model=qwen2.5-1.5b-instruct/compositional',\n",
       " 'encoded': 300,\n",
       " 'rank': '4',\n",
       " 'alpha': '32',\n",
       " 'lr': '0.0003',\n",
       " 'epoch': '4',\n",
       " 'cot_name': 'direct',\n",
       " 'augment_model': 'qwen2.5-1.5b-instruct',\n",
       " 'model_name': 'qwen2.5-1.5b-instruct',\n",
       " 'dataset': '2wikimultihopqa',\n",
       " 'filename': 'compositional'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2624201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 experiment configs\n",
      "                     Dataset         r2_e2         r2_e4         r4_e2  \\\n",
      "0                popqa/total  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]   \n",
      "1             hotpotqa/total  [0, 0, 0, 0]  [0, 0, 0, 0]  [0, 0, 0, 0]   \n",
      "2        hotpotqa/comparison  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]   \n",
      "3            hotpotqa/bridge  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]   \n",
      "4  complexwebquestions/total  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]   \n",
      "\n",
      "          r4_e4         r8_e2         r8_e4        r16_e2        r16_e4  \n",
      "0  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  \n",
      "1  [0, 0, 0, 0]  [0, 0, 0, 0]  [0, 0, 0, 0]  [0, 0, 0, 0]  [0, 0, 0, 0]  \n",
      "2  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  \n",
      "3  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, !, 0, 1]  [1, 1, 0, 1]  [!, 1, 0, 1]  \n",
      "4  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  [1, 1, 0, 1]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "\n",
    "OFFLINE_DIR = \"/scratch/doluk/Compact-Interference-PRAG/offline\"\n",
    "\n",
    "MODELS = [\n",
    "    \"qwen2.5-1.5b-instruct\",\n",
    "    \"llama3.2-1b-instruct\",\n",
    "    \"llama3-8b-instruct\",\n",
    "    \"qwen2.5-0.5b-instruct\",\n",
    "]\n",
    "\n",
    "RANKS = [2, 4, 8, 16]\n",
    "EPOCHS_LIST = [2, 4]\n",
    "r_eps_pair = [(r, e) for r in RANKS for e in EPOCHS_LIST]\n",
    "\n",
    "dataset_filename_dict = {\n",
    "    \"popqa\": [\"total\"],\n",
    "    \"hotpotqa\": [\"total\", \"comparison\", \"bridge\"],\n",
    "    \"complexwebquestions\": [\"total\"],\n",
    "    \"2wikimultihopqa\": [\"total\", \"bridge_comparison\", \"comparison\", \"compositional\", \"inference\"],\n",
    "}\n",
    "\n",
    "EXPECTED_SAMPLES = 300\n",
    "\n",
    "# =========================\n",
    "# Path parsing\n",
    "# =========================\n",
    "\n",
    "def extract_args_from_filename_dir(path: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Extract experiment arguments from:\n",
    "    .../<model>/rank=.../<dataset>/lr=..._epoch=..._direct/aug_model=.../<filename>\n",
    "\n",
    "    Rules:\n",
    "    - skip warm_up\n",
    "    - accept only *_direct\n",
    "    \"\"\"\n",
    "    if \"/warm_up/\" in path:\n",
    "        return None\n",
    "\n",
    "    parts = os.path.normpath(path).split(os.sep)\n",
    "    args = {}\n",
    "\n",
    "    for p in parts:\n",
    "        # rank=2_alpha=32\n",
    "        if p.startswith(\"rank=\"):\n",
    "            for item in p.split(\"_\"):\n",
    "                if \"=\" in item:\n",
    "                    k, v = item.split(\"=\", 1)\n",
    "                    args[k] = int(v)\n",
    "\n",
    "        # lr=0.0003_epoch=4_direct\n",
    "        elif p.startswith(\"lr=\"):\n",
    "            segs = p.split(\"_\")\n",
    "            if len(segs) != 3 or segs[2] != \"direct\":\n",
    "                return None\n",
    "\n",
    "            args[\"learning_rate\"] = float(segs[0].split(\"=\", 1)[1])\n",
    "            args[\"epoch\"] = int(segs[1].split(\"=\", 1)[1])\n",
    "            args[\"cot_name\"] = \"direct\"\n",
    "\n",
    "        # aug_model=...\n",
    "        elif p.startswith(\"aug_model=\"):\n",
    "            args[\"augment_model\"] = p.split(\"=\", 1)[1]\n",
    "\n",
    "    # model_name = directory before rank=\n",
    "    for i, p in enumerate(parts):\n",
    "        if p.startswith(\"rank=\") and i > 0:\n",
    "            args[\"model_name\"] = parts[i - 1]\n",
    "            break\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # dataset = directory after rank=\n",
    "    for i, p in enumerate(parts):\n",
    "        if p.startswith(\"rank=\") and i + 1 < len(parts):\n",
    "            args[\"dataset\"] = parts[i + 1]\n",
    "            break\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # filename (total / comparison / bridge / inference / etc.)\n",
    "    args[\"filename\"] = parts[-1]\n",
    "\n",
    "    return args\n",
    "\n",
    "# =========================\n",
    "# Experiment collection\n",
    "# =========================\n",
    "\n",
    "def collect_experiments(root_dir: str) -> List[Dict]:\n",
    "    experiments = []\n",
    "    seen = set()\n",
    "\n",
    "    for root, dirs, _ in os.walk(root_dir):\n",
    "        data_dirs = [d for d in dirs if d.startswith(\"data_\")]\n",
    "        if not data_dirs:\n",
    "            continue\n",
    "\n",
    "        if root in seen:\n",
    "            continue\n",
    "        seen.add(root)\n",
    "\n",
    "        args = extract_args_from_filename_dir(root)\n",
    "        if args is None:\n",
    "            continue\n",
    "\n",
    "        experiments.append({\n",
    "            \"path\": root,\n",
    "            \"encoded\": len(data_dirs),\n",
    "            **args,\n",
    "        })\n",
    "\n",
    "        # do not descend into data_*\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(\"data_\")]\n",
    "\n",
    "    return experiments\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # -------------------------\n",
    "    # Collect experiments\n",
    "    # -------------------------\n",
    "    experiments = collect_experiments(OFFLINE_DIR)\n",
    "    exp_df = pd.DataFrame(experiments)\n",
    "\n",
    "    print(f\"Found {len(exp_df)} experiment configs\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Build lookup\n",
    "    # -------------------------\n",
    "    # (dataset, filename, rank, epoch, model) -> encoded\n",
    "    exp_lookup = {\n",
    "        (row.dataset, row.filename, row.rank, row.epoch, row.model_name): row.encoded\n",
    "        for row in exp_df.itertuples(index=False)\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # Build tracking table\n",
    "    # -------------------------\n",
    "    dts_fn = [\n",
    "        f\"{dataset}/{fn}\"\n",
    "        for dataset, fns in dataset_filename_dict.items()\n",
    "        for fn in fns\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame({\"Dataset\": dts_fn})\n",
    "\n",
    "    for r, eps in r_eps_pair:\n",
    "        col_name = f\"r{r}_e{eps}\"\n",
    "        col_values = []\n",
    "\n",
    "        for dts in dts_fn:\n",
    "            dataset, filename = dts.split(\"/\")\n",
    "\n",
    "            model_status = []\n",
    "            for model in MODELS:\n",
    "                key = (dataset, filename, r, eps, model)\n",
    "                encoded = exp_lookup.get(key, None)\n",
    "\n",
    "                if encoded is None:\n",
    "                    model_status.append(0)          # not exist\n",
    "                elif encoded < EXPECTED_SAMPLES:\n",
    "                    model_status.append(\"!\")        # incomplete\n",
    "                else:\n",
    "                    model_status.append(1)          # done\n",
    "\n",
    "            col_values.append(model_status)\n",
    "\n",
    "        df[col_name] = col_values\n",
    "\n",
    "    # -------------------------\n",
    "    # Output\n",
    "    # -------------------------\n",
    "    print(df.head())\n",
    "    df.to_csv(\"encoding_progress.csv\", index=False)\n",
    "    df.to_excel('encoding_progress.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
