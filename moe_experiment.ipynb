{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ade8bdc-8c99-4d42-8891-8724519a4758",
   "metadata": {},
   "source": [
    "# Part I: Experiment with trained LoRA-XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0b333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416fb943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88377f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doluk/.conda/envs/prag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c67cfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12748461",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, config = get_model(model_name=\"qwen2.5-1.5b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b4720d1-5548-4021-923a-2c99de7de612",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = r\"/scratch/doluk/Compact-Interference-PRAG/offline_loraxs/qwen2.5-1.5b-instruct/rank=64_alpha=16/popqa/lr=0.0003_epoch=4_direct/aug_model=qwen2.5-1.5b-instruct/total/data_0/passage_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704a606d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adapter_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m----> 2\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[43madapter_path\u001b[49m, is_trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adapter_path' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5b96ce-73ca-415e-ae9f-51554f22a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a3b87b-0b19-49a3-9715-35283d69754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doluk/.conda/envs/prag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/doluk/.conda/envs/prag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/doluk/.conda/envs/prag/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'George Rankin was born into poverty and grew up homeless in Liverpool docks during World War II. He'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generate(\"What is George Rankin's occupation?\", peft_model, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a25d052c-0656-449f-908a-16c068aadfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I couldn't find any information about a person named George Rankin with a\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generate(\"What is George Rankin's occupation?\", model, tokenizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3aa8ee-67e3-4e0d-b568-1f59442edfdc",
   "metadata": {},
   "source": [
    "# Part 2: Testing with MoE architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954b744-6b14-4f5c-83b8-b28d8b4708f5",
   "metadata": {},
   "source": [
    "### Use base Model with separate A/B matrices (frozen pretrained PRAG-baseline adapters), just trainable MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0044f6-a71c-4adc-9d07-e95bd21e98df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "trained_adapter_dir = r\"/scratch/doluk/Compact-Interference-PRAG/offline/qwen2.5-1.5b-instruct/rank=2_alpha=32/popqa/lr=0.0003_epoch=2_direct/aug_model=qwen2.5-1.5b-instruct/total/data_0\"\n",
    "print(os.path.exists(trained_adapter_dir))\n",
    "# print(os.listdir(trained_adapter_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbda3efe-1ceb-41cc-a209-eb154690a5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.26.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.26.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.26.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.26.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.27.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.27.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.27.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.27.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.weight',\n",
       " 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_A.weight',\n",
       " 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_B.weight']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "state = load_file(os.path.join(trained_adapter_dir, \"passage_0\", \"adapter_model.safetensors\"))\n",
    "list(state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a294da02-dd51-40e1-ae37-40a8465c6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# -------------------------------\n",
    "# MoE-LoRA block\n",
    "# -------------------------------\n",
    "class MoELoRA(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, trained_lora_A_weights, trained_lora_B_weights, r=None, alpha=32):\n",
    "        \"\"\"\n",
    "        trained_lora_A_weights/B_weights: list[Tensor] with shapes:\n",
    "          A_i: (r, in_dim)   (down-proj)\n",
    "          B_i: (out_dim, r)  (up-proj)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(trained_lora_A_weights) == len(trained_lora_B_weights)\n",
    "        self.num_heads = len(trained_lora_A_weights)\n",
    "\n",
    "        # infer r if not provided\n",
    "        inferred_r = trained_lora_A_weights[0].shape[0]\n",
    "        if r is None:\n",
    "            r = inferred_r\n",
    "        else:\n",
    "            assert r == inferred_r, f\"Provided r={r} != inferred r={inferred_r}\"\n",
    "\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # A_i: in_dim -> r ; B_i: r -> out_dim\n",
    "        self.As = nn.ModuleList([nn.Linear(in_dim, r, bias=False) for _ in range(self.num_heads)])\n",
    "        self.Bs = nn.ModuleList([nn.Linear(r, out_dim, bias=False) for _ in range(self.num_heads)])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.num_heads):\n",
    "                # Safetensor shapes: A: (r, in_dim) matches Linear(in_dim->r).weight\n",
    "                self.As[i].weight.copy_(trained_lora_A_weights[i])\n",
    "                # Safetensor shapes: B: (out_dim, r) matches Linear(r->out_dim).weight\n",
    "                self.Bs[i].weight.copy_(trained_lora_B_weights[i])\n",
    "\n",
    "        # Simple dense router on a pooled compressed repr\n",
    "        self.router = nn.Linear(r, self.num_heads)\n",
    "\n",
    "        \n",
    "        # Freeze all A/B parameters\n",
    "        for p in self.As.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.Bs.parameters():\n",
    "            p.requires_grad = False\n",
    "        # Ensure router is trainable\n",
    "        for p in self.router.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [..., in_dim]\n",
    "        # compute compressed reps for each head\n",
    "        zs = [A(x) for A in self.As]           # list of [..., r]\n",
    "        z_stack = torch.stack(zs, dim=-1)      # [..., r, num_heads]\n",
    "        # expert outputs\n",
    "        expert_outs = torch.stack(\n",
    "            [self.Bs[i](zs[i]) for i in range(self.num_heads)],\n",
    "            dim=-1\n",
    "        )                                      # [..., out_dim, num_heads]\n",
    "        # router uses pooled compressed repr; stopgrad into A\n",
    "        z_mean = z_stack.mean(dim=-1)          # [..., r]\n",
    "        router_logits = self.router(z_mean.detach())\n",
    "        router_scores = F.softmax(router_logits, dim=-1)  # [..., num_heads]\n",
    "        # mix experts\n",
    "        out = torch.sum(expert_outs * router_scores.unsqueeze(-2), dim=-1)  # [..., out_dim]\n",
    "        return self.scaling * out\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Wrapper Linear Layer\n",
    "# -------------------------------\n",
    "class LinearWithMoELoRA(nn.Module):\n",
    "    def __init__(self, base_linear: nn.Linear, moe_lora: MoELoRA):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        self.moe_lora = moe_lora\n",
    "        # base stays frozen (you can unfreeze if you want full finetuning)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            base_out = self.base(x)  # base is frozen; grads won't flow into it\n",
    "        return base_out + self.moe_lora(x)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Injection Utilities\n",
    "# -------------------------------\n",
    "TARGET_MODULES = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "def _collect_passage_paths(trained_data_adapters_dir: str) -> List[Path]:\n",
    "    root = Path(trained_data_adapters_dir)\n",
    "    assert root.exists(), f\"Adapters dir not found: {root}\"\n",
    "    passages = sorted([p for p in root.iterdir() if p.is_dir() and p.name.startswith(\"passage_\")],\n",
    "                      key=lambda p: int(p.name.split(\"_\")[-1]))\n",
    "    assert len(passages) > 0, f\"No passage_* folders found under {root}\"\n",
    "    return passages\n",
    "\n",
    "def _load_all_adapter_states(passages: List[Path]) -> List[Dict[str, torch.Tensor]]:\n",
    "    states = []\n",
    "    for p in passages:\n",
    "        st_path = p / \"adapter_model.safetensors\"\n",
    "        assert st_path.exists(), f\"Missing {st_path}\"\n",
    "        states.append(load_file(str(st_path)))\n",
    "    return states\n",
    "\n",
    "def _find_weights_for_module(\n",
    "    module_name: str,\n",
    "    adapter_states: List[Dict[str, torch.Tensor]],\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    For a model module name like 'model.layers.0.mlp.down_proj',\n",
    "    find matching safetensor keys ending with '.{module_name}.lora_A.weight' / B across all passages.\n",
    "    Returns lists A_list, B_list aligned by passage order.\n",
    "    \"\"\"\n",
    "    suffix_A = f\".{module_name}.lora_A.weight\"\n",
    "    suffix_B = f\".{module_name}.lora_B.weight\"\n",
    "\n",
    "    A_list, B_list = [], []\n",
    "    for st in adapter_states:\n",
    "        # keys have long prefixes; we match by endswith\n",
    "        kA = next((k for k in st.keys() if k.endswith(suffix_A)), None)\n",
    "        kB = next((k for k in st.keys() if k.endswith(suffix_B)), None)\n",
    "        if kA is None or kB is None:\n",
    "            # Fallback: keys might omit the leading 'model.' in module_name\n",
    "            alt_suffix_A = \".\" + module_name.split(\"model.\", 1)[-1] + \".lora_A.weight\"\n",
    "            alt_suffix_B = \".\" + module_name.split(\"model.\", 1)[-1] + \".lora_B.weight\"\n",
    "            kA = next((k for k in st.keys() if k.endswith(alt_suffix_A)), kA)\n",
    "            kB = next((k for k in st.keys() if k.endswith(alt_suffix_B)), kB)\n",
    "        if kA is None or kB is None:\n",
    "            # If still missing, we cannot inject for this module from this passage\n",
    "            return [], []\n",
    "        A_list.append(st[kA])\n",
    "        B_list.append(st[kB])\n",
    "    return A_list, B_list\n",
    "\n",
    "def _replace_module(model: nn.Module, dotted_name: str, new_mod: nn.Module):\n",
    "    \"\"\"\n",
    "    Replace submodule at dotted_name with new_mod.\n",
    "    \"\"\"\n",
    "    parent_name, attr = dotted_name.rsplit('.', 1)\n",
    "    parent = dict(model.named_modules())[parent_name]\n",
    "    setattr(parent, attr.split(':')[0], new_mod)  # strip any :type suffix just in case\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main injector\n",
    "# -------------------------------\n",
    "def inject_hydra_lora(\n",
    "    model: nn.Module,\n",
    "    trained_data_adapters_dir: str,\n",
    "    r: int = None,\n",
    "    alpha: int = 32,\n",
    "    target_modules: List[str] = TARGET_MODULES,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each Linear in TARGET_MODULES, wrap it with LinearWithMoELoRA\n",
    "    using the LoRA A/B from all 'passage_*' adapters found in the dir.\n",
    "    \"\"\"\n",
    "    # 1) Load all passage adapters (each passage = one expert head)\n",
    "    passages = _collect_passage_paths(trained_data_adapters_dir)\n",
    "    adapter_states = _load_all_adapter_states(passages)\n",
    "    if verbose:\n",
    "        print(f\"Found {len(adapter_states)} passages: {[p.name for p in passages]}\")\n",
    "\n",
    "    # 2) Freeze base model\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 3) Traverse modules and inject\n",
    "    injected = 0\n",
    "    # Create a stable list of (name, module) so we can replace during iteration\n",
    "    modules = [(n, m) for n, m in model.named_modules() if isinstance(m, nn.Linear)]\n",
    "    for name, module in modules:\n",
    "        if not any(t in name for t in target_modules):\n",
    "            continue\n",
    "\n",
    "        # Find A/B weights across passages for this module name\n",
    "        A_list, B_list = _find_weights_for_module(name, adapter_states)\n",
    "        if len(A_list) == 0:\n",
    "            if verbose:\n",
    "                print(f\"[skip] No matching LoRA weights for {name}\")\n",
    "            continue\n",
    "\n",
    "        in_dim, out_dim = module.in_features, module.out_features\n",
    "\n",
    "        # Sanity checks\n",
    "        for i, (A, B) in enumerate(zip(A_list, B_list)):\n",
    "            assert A.shape[1] == in_dim, f\"{name} passage#{i} A shape {A.shape} incompatible with in_dim={in_dim}\"\n",
    "            assert B.shape[0] == out_dim, f\"{name} passage#{i} B shape {B.shape} incompatible with out_dim={out_dim}\"\n",
    "            assert A.shape[0] == B.shape[1], f\"{name} passage#{i} r mismatch: {A.shape[0]} vs {B.shape[1]}\"\n",
    "\n",
    "        # Instantiate MoE-LoRA for this linear\n",
    "        moe = MoELoRA(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            trained_lora_A_weights=A_list,\n",
    "            trained_lora_B_weights=B_list,\n",
    "            r=(A_list[0].shape[0] if r is None else r),\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        wrapped = LinearWithMoELoRA(module, moe)\n",
    "\n",
    "        # Replace module in model\n",
    "        _replace_module(model, name, wrapped)\n",
    "        injected += 1\n",
    "        if verbose:\n",
    "            r_used = moe.r\n",
    "            print(f\"[ok] Injected MoE-LoRA into {name} (heads={len(A_list)}, r={r_used}, alpha={alpha})\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Done. Injected {injected} modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f12817f1-ad9d-4e43-a3c9-ca30049741c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56b3fbd7-cdb0-42eb-861c-dfc7d8fcf478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "TARGET_MODULES = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if any(target in name for target in TARGET_MODULES) and isinstance(module, nn.Linear):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c682e439-7260-4937-b5b2-8a0e62b022b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 passages: ['passage_0', 'passage_1', 'passage_2']\n",
      "[ok] Injected MoE-LoRA into model.layers.0.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.0.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.0.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.1.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.1.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.1.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.2.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.2.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.2.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.3.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.3.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.3.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.4.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.4.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.4.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.5.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.5.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.5.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.6.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.6.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.6.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.7.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.7.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.7.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.8.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.8.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.8.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.9.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.9.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.9.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.10.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.10.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.10.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.11.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.11.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.11.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.12.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.12.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.12.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.13.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.13.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.13.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.14.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.14.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.14.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.15.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.15.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.15.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.16.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.16.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.16.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.17.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.17.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.17.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.18.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.18.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.18.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.19.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.19.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.19.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.20.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.20.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.20.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.21.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.21.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.21.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.22.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.22.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.22.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.23.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.23.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.23.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.24.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.24.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.24.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.25.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.25.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.25.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.26.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.26.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.26.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.27.mlp.gate_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.27.mlp.up_proj (heads=3, r=2, alpha=32)\n",
      "[ok] Injected MoE-LoRA into model.layers.27.mlp.down_proj (heads=3, r=2, alpha=32)\n",
      "Done. Injected 84 modules.\n"
     ]
    }
   ],
   "source": [
    "inject_hydra_lora(model, trained_data_adapters_dir=trained_adapter_dir, alpha=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c1a7cc-4b2e-4d5f-9bbe-adf1fb8e782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closed_book_QA(aug_model, augments, tokenizer, args=None):\n",
    "    from prompt_template import get_prompt\n",
    "    prompt_ids = []\n",
    "    for aug in augments:\n",
    "        rew = aug[f\"{aug_model}_rewrite\"]\n",
    "        qas = aug[f\"{aug_model}_qa\"]\n",
    "        qas = qas\n",
    "        qpa_cnt = (len(qas) + 1) // 2\n",
    "        for qid, qa in enumerate(qas):\n",
    "            prompt_ids.append(get_prompt(tokenizer, qa[\"question\"], None, qa[\"answer\"]))\n",
    "    return prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fa8592c-393c-4cac-871e-5f376db0b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_file = r\"/scratch/doluk/Compact-Interference-PRAG/data_aug/popqa/qwen2.5-1.5b-instruct/total.json\"\n",
    "with open(data_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "augments = data[0][\"augment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34856ce1-00ae-465e-abe2-5141e24c12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = \"qwen2.5-1.5b-instruct\"\n",
    "prompt_ids = get_closed_book_QA(aug_model, augments, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b10d8a-1780-4ed6-a958-0b9838e83200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e11357a2-4062-4465-bc5b-f5a7a1aa9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf = [p.shape for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b33c0884-1329-46e9-92a3-dfb3f5d633c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "773280e4-5781-4d00-81e6-24e3661bfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encode import get_train_data, TrainingData, TrainingDataCollator\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "def train(question, augments, model, tokenizer, save_path=\"please_first_test\"):\n",
    "    prompt_ids = get_closed_book_QA(aug_model, augments, tokenizer)\n",
    "    train_data = TrainingData(prompt_ids, tokenizer)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=1,\n",
    "        collate_fn=TrainingDataCollator(tokenizer, model.device),\n",
    "        shuffle=False,\n",
    "    )\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model_parameters, lr=3e-4)\n",
    "    for epoch in range(4):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # print(f\"Save LoRA to {save_path}\")\n",
    "    model.save_pretrained(save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1491132a-4551-4f02-8583-eb09c20c3974",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacty of 39.50 GiB of which 119.38 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.03 GiB is allocated by PyTorch, and 875.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is George Rankin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms occupation?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(question, augments, model, tokenizer, save_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1104\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1101\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1118\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:915\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    904\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    905\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    906\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m         cache_position,\n\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:669\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    668\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    672\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:223\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(hidden_state)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 93\u001b[0m, in \u001b[0;36mLinearWithMoELoRA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     92\u001b[0m     base_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase(x)  \u001b[38;5;66;03m# base is frozen; grads won't flow into it\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m base_out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoe_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 74\u001b[0m, in \u001b[0;36mMoELoRA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m router_scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(router_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [..., num_heads]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# mix experts\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mexpert_outs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrouter_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [..., out_dim]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling \u001b[38;5;241m*\u001b[39m out\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacty of 39.50 GiB of which 119.38 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 38.03 GiB is allocated by PyTorch, and 875.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = train(\"What is George Rankin's occupation?\", augments, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1849e9b1-4013-472f-9f17-13f0f5cac698",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '/home/doluk/Compact-Interference-PRAG/please_first_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/peft/config.py:205\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/doluk/Compact-Interference-PRAG/please_first_test'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m base \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     base_model,\n\u001b[1;32m     14\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     15\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load your fine-tuned adapters\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Merge LoRA weights if you want a single model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/peft/peft_model.py:484\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 484\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    494\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m~/.conda/envs/prag/lib/python3.10/site-packages/peft/config.py:211\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    206\u001b[0m             model_id,\n\u001b[1;32m    207\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    208\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/home/doluk/Compact-Interference-PRAG/please_first_test'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "base_model = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "adapter_path = \"/home/doluk/Compact-Interference-PRAG/please_first_test\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load base model\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# Load your fine-tuned adapters\n",
    "model = PeftModel.from_pretrained(base, adapter_path)\n",
    "\n",
    "# Merge LoRA weights if you want a single model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model (optional)\n",
    "model.save_pretrained(\"/home/doluk/Compact-Interference-PRAG/merged_model\")\n",
    "tokenizer.save_pretrained(\"/home/doluk/Compact-Interference-PRAG/merged_model\")\n",
    "\n",
    "# Try inference\n",
    "question = \"What is George Rankin's occupation?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
